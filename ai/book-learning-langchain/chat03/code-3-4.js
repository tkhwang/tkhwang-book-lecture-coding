const { ChatPromptTemplate } = require("@langchain/core/prompts");
const { ChatOpenAI } = require("@langchain/openai");
import { RunnableLambda } from '@langchain/core/runnables';
import 'dotenv/config'
const { db } = await import("./code-3-1.js")

const retriever = db.asRetriever();

const prompt = ChatPromptTemplate.fromTemplate(
    'ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n ì»¨í…ìŠ¤íŠ¸: {context}\n ì§ˆë¬¸: {question}\n ë‹µë³€:'
)

const llm = new ChatOpenAI({ temperature: 0, modelName: "gpt-4o-mini", apiKey: process.env.OPENAI_API_KEY })

// const chain = prompt.pipe(llm)

// const result = await chain.invoke({
//     context: docs,
//     question: query
// })

const qa = RunnableLambda.from(async (input) => {
    const docs = await retriever.invoke(input);
    const formatted = await prompt.invoke({ context: docs, question: input })
    const answer = await llm.invoke(formatted);
    return answer
})

const finalResult = await qa.invoke("AI agent")
console.log("ðŸš€ ~ finalResult:", finalResult)