{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXsfYVCzoZ84"
      },
      "source": [
        "# 7. LangSmith를 활용한 RAG 애플리케이션 평가\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
          "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
          "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
          "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
        },
        "id": "558jptEWoZ85"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hQV0tspoZ86"
      },
      "source": [
        "## 7.4. Ragas를 활용한 합성 테스트 데이터 생성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccowWgLPoZ86"
      },
      "source": [
        "### 패키지 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5PO1hz4oZ86"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core==0.2.30 langchain-openai==0.1.21 \\\n",
        "    langchain-community==0.2.12 GitPython==3.1.43 \\\n",
        "    langchain-chroma==0.1.2 chromadb==0.5.3 \\\n",
        "    ragas==0.1.14 nest-asyncio==1.6.0 pydantic==2.9.2 numpy==1.25.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XosBAJ1oZ86"
      },
      "source": [
        "### 검색 대상 문서 로드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfrs5iUSoZ86"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import GitLoader\n",
        "\n",
        "\n",
        "def file_filter(file_path: str) -> bool:\n",
        "    return file_path.endswith(\".mdx\")\n",
        "\n",
        "\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
        "    repo_path=\"./langchain\",\n",
        "    branch=\"master\",\n",
        "    file_filter=file_filter,\n",
        ")\n",
        "\n",
        "documents = loader.load()\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zl7xcs5oZ86"
      },
      "source": [
        "### Ragas를 활용한 합성 테스트 데이터 생성 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f878Y37oZ87"
      },
      "outputs": [],
      "source": [
        "for document in documents:\n",
        "    document.metadata[\"filename\"] = document.metadata[\"source\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzFdSJhooZ87"
      },
      "source": [
        "#### 【주의】알려진 오류에 관해\n",
        "\n",
        "아래 코드에서 gpt-4o를 사용할 경우 OpenAI API의 Usage tier에 따라 RateLimitError가 발생할 수 있다고 보고되었습니다.\n",
        "\n",
        "OpenAI API의 Usage tier에 관해서는 공식 문서의 다음 페이지를 참조하세요.\n",
        "\n",
        "https://platform.openai.com/docs/guides/rate-limits/usage-tiers\n",
        "\n",
        "이 오류가 발생한 경우 다음 중 하나의 대응을 실시하세요.\n",
        "\n",
        "1. 같은 Tier에서도 gpt-4o보다 레이트 리밋이 높은 gpt-4o-mini를 사용한다\n",
        "   - 이 경우, 생성되는 합성 테스트 데이터의 품질이 낮아질 것으로 예상됩니다\n",
        "2. 과금 등을 통해 Tier를 올린다\n",
        "   - Tier 2에서는 RateLimitError가 발생하지 않는 것을 확인했습니다 (2024년 10월 31일 기준)\n",
        "\n",
        "##### 2025/3/15 추가\n",
        "\n",
        "LangChain 문서의 증가로 인해, gpt-4o-mini를 사용하더라도 Tier 1에서는 오류가 발생한다는 보고가 있습니다.\n",
        "\n",
        "이 경우, GitHub에서 문서를 로드하는 부분에서 다음과 같이 작동이 확인된 버전인 `langchain==0.2.13`을 지정하도록 하세요.\n",
        "\n",
        "```python\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
        "    repo_path=\"./langchain\",\n",
        "    branch=\"langchain==0.2.13\",\n",
        "    file_filter=file_filter,\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARH4WhZgoZ87"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm=ChatOpenAI(model=\"gpt-4o\"),\n",
        "    critic_llm=ChatOpenAI(model=\"gpt-4o\"),\n",
        "    embeddings=OpenAIEmbeddings(),\n",
        ")\n",
        "\n",
        "testset = generator.generate_with_langchain_docs(\n",
        "    documents,\n",
        "    test_size=4,\n",
        "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pTWmwRSoZ87"
      },
      "outputs": [],
      "source": [
        "testset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PNRkXu0oZ87"
      },
      "source": [
        "### LangSmith의 Dataset 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv286-93oZ87"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "dataset_name = \"agent-book\"\n",
        "\n",
        "client = Client()\n",
        "\n",
        "if client.has_dataset(dataset_name=dataset_name):\n",
        "    client.delete_dataset(dataset_name=dataset_name)\n",
        "\n",
        "dataset = client.create_dataset(dataset_name=dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNjCCMQHoZ87"
      },
      "source": [
        "### 합성 테스트 데이터 저장\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J1Z1qkboZ87"
      },
      "outputs": [],
      "source": [
        "inputs = []\n",
        "outputs = []\n",
        "metadatas = []\n",
        "\n",
        "for testset_record in testset.test_data:\n",
        "    inputs.append(\n",
        "        {\n",
        "            \"question\": testset_record.question,\n",
        "        }\n",
        "    )\n",
        "    outputs.append(\n",
        "        {\n",
        "            \"contexts\": testset_record.contexts,\n",
        "            \"ground_truth\": testset_record.ground_truth,\n",
        "        }\n",
        "    )\n",
        "    metadatas.append(\n",
        "        {\n",
        "            \"source\": testset_record.metadata[0][\"source\"],\n",
        "            \"evolution_type\": testset_record.evolution_type,\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtaCO3vloZ87"
      },
      "outputs": [],
      "source": [
        "client.create_examples(\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    metadata=metadatas,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVmP4yOToZ88"
      },
      "source": [
        "## 7.5. LangSmith와 Ragas를 활용한 오프라인 평가 구현\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScujV403oZ88"
      },
      "source": [
        "### 커스텀 Evaluator 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BikeKwOzoZ88"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langsmith.schemas import Example, Run\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics.base import Metric, MetricWithEmbeddings, MetricWithLLM\n",
        "\n",
        "\n",
        "class RagasMetricEvaluator:\n",
        "    def __init__(self, metric: Metric, llm: BaseChatModel, embeddings: Embeddings):\n",
        "        self.metric = metric\n",
        "\n",
        "        # LLM과 Embeddings을 Metric에 설정\n",
        "        if isinstance(self.metric, MetricWithLLM):\n",
        "            self.metric.llm = LangchainLLMWrapper(llm)\n",
        "        if isinstance(self.metric, MetricWithEmbeddings):\n",
        "            self.metric.embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
        "\n",
        "    def evaluate(self, run: Run, example: Example) -> dict[str, Any]:\n",
        "        context_strs = [doc.page_content for doc in run.outputs[\"contexts\"]]\n",
        "\n",
        "        # Ragas의 평가 메트릭의 score 메서드로 점수 산출\n",
        "        score = self.metric.score(\n",
        "            {\n",
        "                \"question\": example.inputs[\"question\"],\n",
        "                \"answer\": run.outputs[\"answer\"],\n",
        "                \"contexts\": context_strs,\n",
        "                \"ground_truth\": example.outputs[\"ground_truth\"],\n",
        "            },\n",
        "        )\n",
        "        return {\"key\": self.metric.name, \"score\": score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUtosqw-oZ88"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from ragas.metrics import answer_relevancy, context_precision\n",
        "\n",
        "metrics = [context_precision, answer_relevancy]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "evaluators = [\n",
        "    RagasMetricEvaluator(metric, llm, embeddings).evaluate\n",
        "    for metric in metrics\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAeAfETvoZ88"
      },
      "source": [
        "### 추론 함수 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_f6awmboZ88"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "db = Chroma.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtUqYHs3oZ88"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\\\n",
        "다음 문맥만을 고려해 질문에 답하세요.\n",
        "\n",
        "문맥: \"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "질문: {question}\n",
        "''')\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "chain = RunnableParallel(\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"context\": retriever,\n",
        "    }\n",
        ").assign(answer=prompt | model | StrOutputParser())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmPp6oL5oZ88"
      },
      "outputs": [],
      "source": [
        "def predict(inputs: dict[str, Any]) -> dict[str, Any]:\n",
        "    question = inputs[\"question\"]\n",
        "    output = chain.invoke(question)\n",
        "    return {\n",
        "        \"contexts\": output[\"context\"],\n",
        "        \"answer\": output[\"answer\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr6sF78CoZ88"
      },
      "source": [
        "### 오프라인 평가 구현·실행\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXG0SOQeoZ88"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "evaluate(\n",
        "    predict,\n",
        "    data=\"agent-book\",\n",
        "    evaluators=evaluators,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOXerRwyoZ89"
      },
      "source": [
        "## LangSmith를 활용한 온라인 평가 구현\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwu4vrx3oZ89"
      },
      "source": [
        "### 피드백 버튼을 표시하는 함수 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxc3F8pyoZ89"
      },
      "outputs": [],
      "source": [
        "from uuid import UUID\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from langsmith import Client\n",
        "\n",
        "\n",
        "def display_feedback_buttons(run_id: UUID) -> None:\n",
        "    # Good 버튼과 Bad 버튼 준비\n",
        "    good_button = widgets.Button(\n",
        "        description=\"Good\",\n",
        "        button_style=\"success\",\n",
        "        icon=\"thumbs-up\",\n",
        "    )\n",
        "    bad_button = widgets.Button(\n",
        "        description=\"Bad\",\n",
        "        button_style=\"danger\",\n",
        "        icon=\"thumbs-down\",\n",
        "    )\n",
        "\n",
        "    # 클릭 시 실행될 함수 정의\n",
        "    def on_button_clicked(button: widgets.Button) -> None:\n",
        "        if button == good_button:\n",
        "            score = 1\n",
        "        elif button == bad_button:\n",
        "            score = 0\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown button: {button}\")\n",
        "\n",
        "        client = Client()\n",
        "        client.create_feedback(run_id=run_id, key=\"thumbs\", score=score)\n",
        "        print(\"피드백을 전송했습니다\")\n",
        "\n",
        "    # 버튼 클릭 시 on_button_clicked 함수 실행\n",
        "    good_button.on_click(on_button_clicked)\n",
        "    bad_button.on_click(on_button_clicked)\n",
        "\n",
        "    # 버튼 표시\n",
        "    display(good_button, bad_button)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is8nKteHoZ89"
      },
      "source": [
        "### 피드백 버튼 표시\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzqyu0aDoZ89"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tracers.context import collect_runs\n",
        "\n",
        "# LangSmith의 트레이스 ID(Run ID)를 가져오기 위해 collect_runs 함수 사용\n",
        "with collect_runs() as runs_cb:\n",
        "    output = chain.invoke(\"LangChain의 개요를 알려줘\")\n",
        "    print(output[\"answer\"])\n",
        "    run_id = runs_cb.traced_runs[0].id\n",
        "\n",
        "display_feedback_buttons(run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0gW491DoZ89"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}