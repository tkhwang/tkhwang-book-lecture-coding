{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x7b3Rpfgpk0"
      },
      "source": [
        "# 5. LangChain Expression Language(LCEL) 심층 해설\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
          "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
          "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
          "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
        },
        "id": "jvuKOwNbgpk1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ4fwamMgpk1"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core==0.3.0 langchain-openai==0.2.0 langchain-community==0.3.0 pydantic==2.10.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkvQCCUQgpk1"
      },
      "source": [
        "## 5.1. Runnable과 RunnableSequence―LCEL의 가장 기본적인 구성 요소\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:12.290335Z",
          "iopub.status.busy": "2024-06-28T02:33:12.290156Z",
          "iopub.status.idle": "2024-06-28T02:33:12.344661Z",
          "shell.execute_reply": "2024-06-28T02:33:12.344241Z"
        },
        "id": "iw47kajfgpk2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:12.346689Z",
          "iopub.status.busy": "2024-06-28T02:33:12.346510Z",
          "iopub.status.idle": "2024-06-28T02:33:21.108437Z",
          "shell.execute_reply": "2024-06-28T02:33:21.108007Z"
        },
        "id": "v1e4ytz_gpk2"
      },
      "outputs": [],
      "source": [
        "prompt_value = prompt.invoke({\"dish\": \"카레\"})\n",
        "ai_message = model.invoke(prompt_value)\n",
        "output = output_parser.invoke(ai_message)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:21.110332Z",
          "iopub.status.busy": "2024-06-28T02:33:21.110173Z",
          "iopub.status.idle": "2024-06-28T02:33:21.112634Z",
          "shell.execute_reply": "2024-06-28T02:33:21.112238Z"
        },
        "id": "Y1_uIxQ4gpk2"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:21.114678Z",
          "iopub.status.busy": "2024-06-28T02:33:21.114418Z",
          "iopub.status.idle": "2024-06-28T02:33:26.539851Z",
          "shell.execute_reply": "2024-06-28T02:33:26.539250Z"
        },
        "id": "sidnFryBgpk2"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke({\"dish\": \"카레\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyCfUHUngpk2"
      },
      "source": [
        "### Runnable의 실행 방법―invoke・stream・batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:26.545129Z",
          "iopub.status.busy": "2024-06-28T02:33:26.544905Z",
          "iopub.status.idle": "2024-06-28T02:33:32.478679Z",
          "shell.execute_reply": "2024-06-28T02:33:32.478134Z"
        },
        "id": "G44R6pakgpk3"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | output_parser\n",
        "\n",
        "for chunk in chain.stream({\"dish\": \"카레\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:32.480592Z",
          "iopub.status.busy": "2024-06-28T02:33:32.480406Z",
          "iopub.status.idle": "2024-06-28T02:33:38.976064Z",
          "shell.execute_reply": "2024-06-28T02:33:38.974376Z"
        },
        "id": "2pol6DGIgpk3"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | output_parser\n",
        "\n",
        "outputs = chain.batch([{\"dish\": \"카레\"}, {\"dish\": \"우동\"}])\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2QALc2_gpk3"
      },
      "source": [
        "### LCEL의 \"|\"로 다양한 Runnable 연결하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:38.984366Z",
          "iopub.status.busy": "2024-06-28T02:33:38.983620Z",
          "iopub.status.idle": "2024-06-28T02:33:39.072077Z",
          "shell.execute_reply": "2024-06-28T02:33:39.071585Z"
        },
        "id": "f3Djwd2Tgpk3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:39.073954Z",
          "iopub.status.busy": "2024-06-28T02:33:39.073805Z",
          "iopub.status.idle": "2024-06-28T02:33:39.076746Z",
          "shell.execute_reply": "2024-06-28T02:33:39.076291Z"
        },
        "id": "dCC0nUongpk3"
      },
      "outputs": [],
      "source": [
        "cot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"사용자의 질문에 단계적으로 답변하세요.\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cot_chain = cot_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:39.078597Z",
          "iopub.status.busy": "2024-06-28T02:33:39.078287Z",
          "iopub.status.idle": "2024-06-28T02:33:39.080804Z",
          "shell.execute_reply": "2024-06-28T02:33:39.080464Z"
        },
        "id": "26AxQ2Pygpk3"
      },
      "outputs": [],
      "source": [
        "summarize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"단계적으로 생각한 답변에서 결론만 추출하세요.\"),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "summarize_chain = summarize_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:39.082487Z",
          "iopub.status.busy": "2024-06-28T02:33:39.082344Z",
          "iopub.status.idle": "2024-06-28T02:33:42.144944Z",
          "shell.execute_reply": "2024-06-28T02:33:42.144538Z"
        },
        "id": "VDRZWSxqgpk3"
      },
      "outputs": [],
      "source": [
        "cot_summarize_chain = cot_chain | summarize_chain\n",
        "output = cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu5nZTZFgpk3"
      },
      "source": [
        "## 5.2. RunnableLambda―임의의 함수를 Runnable로 만들기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:42.146898Z",
          "iopub.status.busy": "2024-06-28T02:33:42.146736Z",
          "iopub.status.idle": "2024-06-28T02:33:42.204154Z",
          "shell.execute_reply": "2024-06-28T02:33:42.203681Z"
        },
        "id": "POlYJCm4gpk4"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:42.206082Z",
          "iopub.status.busy": "2024-06-28T02:33:42.205909Z",
          "iopub.status.idle": "2024-06-28T02:33:43.030226Z",
          "shell.execute_reply": "2024-06-28T02:33:43.029756Z"
        },
        "id": "y8DJj3qvgpk4"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | output_parser | RunnableLambda(upper)\n",
        "\n",
        "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjfB51xjgpk4"
      },
      "source": [
        "### chain 데코레이터를 사용한 RunnableLambda 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.032211Z",
          "iopub.status.busy": "2024-06-28T02:33:43.032048Z",
          "iopub.status.idle": "2024-06-28T02:33:43.501555Z",
          "shell.execute_reply": "2024-06-28T02:33:43.499283Z"
        },
        "id": "6A7mxm6ugpk4"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import chain\n",
        "\n",
        "\n",
        "@chain\n",
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | output_parser | upper\n",
        "\n",
        "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJCe5l0Ngpk4"
      },
      "source": [
        "### RunnableLambda 자동 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.508518Z",
          "iopub.status.busy": "2024-06-28T02:33:43.508321Z",
          "iopub.status.idle": "2024-06-28T02:33:43.511080Z",
          "shell.execute_reply": "2024-06-28T02:33:43.510672Z"
        },
        "id": "xTSxNdrFgpk4"
      },
      "outputs": [],
      "source": [
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | output_parser | upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.512885Z",
          "iopub.status.busy": "2024-06-28T02:33:43.512594Z",
          "iopub.status.idle": "2024-06-28T02:33:43.961318Z",
          "shell.execute_reply": "2024-06-28T02:33:43.960803Z"
        },
        "id": "kcZUxBzegpk4"
      },
      "outputs": [],
      "source": [
        "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEZdX1idgpk4"
      },
      "source": [
        "### Runnable의 입력 타입과 출력 타입에 주의\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.963174Z",
          "iopub.status.busy": "2024-06-28T02:33:43.963026Z",
          "iopub.status.idle": "2024-06-28T02:33:43.965674Z",
          "shell.execute_reply": "2024-06-28T02:33:43.965305Z"
        },
        "id": "BgXWvKxSgpk4"
      },
      "outputs": [],
      "source": [
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | upper\n",
        "\n",
        "# 아래 코드를 실행하면 오류가 발생합니다\n",
        "output = chain.invoke({\"input\": \"Hello!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.967542Z",
          "iopub.status.busy": "2024-06-28T02:33:43.967246Z",
          "iopub.status.idle": "2024-06-28T02:33:44.531734Z",
          "shell.execute_reply": "2024-06-28T02:33:44.531210Z"
        },
        "id": "maAuLYwXgpk4"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | StrOutputParser() | upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zatE6tdcgpk5"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyx---Y6gpk5"
      },
      "source": [
        "### (칼럼) 사용자 함수를 stream에 대응시키는 방법\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:44.533838Z",
          "iopub.status.busy": "2024-06-28T02:33:44.533678Z",
          "iopub.status.idle": "2024-06-28T02:33:45.353621Z",
          "shell.execute_reply": "2024-06-28T02:33:45.353115Z"
        },
        "id": "uMj9etfGgpk5"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator\n",
        "\n",
        "\n",
        "def upper(input_stream: Iterator[str]) -> Iterator[str]:\n",
        "    for text in input_stream:\n",
        "        yield text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | StrOutputParser() | upper\n",
        "\n",
        "for chunk in chain.stream({\"input\": \"Hello!\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BklIITh1gpk5"
      },
      "source": [
        "## 5.3. RunnableParallel―여러 Runnable을 병렬로 처리하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.355560Z",
          "iopub.status.busy": "2024-06-28T02:33:45.355402Z",
          "iopub.status.idle": "2024-06-28T02:33:45.407879Z",
          "shell.execute_reply": "2024-06-28T02:33:45.407407Z"
        },
        "id": "q69b2WNfgpk5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.409670Z",
          "iopub.status.busy": "2024-06-28T02:33:45.409524Z",
          "iopub.status.idle": "2024-06-28T02:33:45.412232Z",
          "shell.execute_reply": "2024-06-28T02:33:45.411889Z"
        },
        "id": "vDdo367ogpk5"
      },
      "outputs": [],
      "source": [
        "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 낙관주의자입니다. 사용자의 입력에 대해 낙관적인 의견을 제공하세요.\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")\n",
        "optimistic_chain = optimistic_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.414087Z",
          "iopub.status.busy": "2024-06-28T02:33:45.413807Z",
          "iopub.status.idle": "2024-06-28T02:33:45.416778Z",
          "shell.execute_reply": "2024-06-28T02:33:45.416359Z"
        },
        "id": "_96vanPzgpk5"
      },
      "outputs": [],
      "source": [
        "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 비관주의자입니다. 사용자의 입력에 대해 비관적인 의견을 제공하세요.\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")\n",
        "pessimistic_chain = pessimistic_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.418636Z",
          "iopub.status.busy": "2024-06-28T02:33:45.418458Z",
          "iopub.status.idle": "2024-06-28T02:33:48.115947Z",
          "shell.execute_reply": "2024-06-28T02:33:48.115444Z"
        },
        "id": "4jE7_Kg5gpk5"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "parallel_chain = RunnableParallel(\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "    }\n",
        ")\n",
        "\n",
        "output = parallel_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "pprint.pprint(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh_836fMgpk6"
      },
      "source": [
        "### RunnableParallel의 출력을 Runnable의 입력으로 연결하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:48.117879Z",
          "iopub.status.busy": "2024-06-28T02:33:48.117728Z",
          "iopub.status.idle": "2024-06-28T02:33:54.979992Z",
          "shell.execute_reply": "2024-06-28T02:33:54.978363Z"
        },
        "id": "HnU_2vpvgpk6"
      },
      "outputs": [],
      "source": [
        "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 객관적 AI입니다. 두 가지 의견을 종합하세요.\"),\n",
        "        (\"human\", \"낙관적 의견: {optimistic_opinion}\\n비관적 의견: {pessimistic_opinion}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZf4LL47gpk6"
      },
      "outputs": [],
      "source": [
        "synthesize_chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"optimistic_opinion\": optimistic_chain,\n",
        "            \"pessimistic_opinion\": pessimistic_chain,\n",
        "        }\n",
        "    )\n",
        "    | synthesize_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COKJXWrsgpk6"
      },
      "source": [
        "### RunnableParallel 자동 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:54.986651Z",
          "iopub.status.busy": "2024-06-28T02:33:54.986098Z",
          "iopub.status.idle": "2024-06-28T02:33:54.994428Z",
          "shell.execute_reply": "2024-06-28T02:33:54.992735Z"
        },
        "id": "TycHWzFegpk6"
      },
      "outputs": [],
      "source": [
        "synthesize_chain = (\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "    }\n",
        "    | synthesize_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:55.000866Z",
          "iopub.status.busy": "2024-06-28T02:33:55.000312Z",
          "iopub.status.idle": "2024-06-28T02:34:01.170210Z",
          "shell.execute_reply": "2024-06-28T02:34:01.169705Z"
        },
        "id": "B2tXTmJegpk7"
      },
      "outputs": [],
      "source": [
        "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNN-x47bgpk7"
      },
      "source": [
        "### RunnableLambda와의 조합―itemgetter를 사용한 예시\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:01.172136Z",
          "iopub.status.busy": "2024-06-28T02:34:01.171982Z",
          "iopub.status.idle": "2024-06-28T02:34:01.174756Z",
          "shell.execute_reply": "2024-06-28T02:34:01.174370Z"
        },
        "id": "_0CLV_6sgpk7"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "topic_getter = itemgetter(\"topic\")\n",
        "topic = topic_getter({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:01.176493Z",
          "iopub.status.busy": "2024-06-28T02:34:01.176278Z",
          "iopub.status.idle": "2024-06-28T02:34:09.682638Z",
          "shell.execute_reply": "2024-06-28T02:34:09.682146Z"
        },
        "id": "mXj7XLL5gpk7"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"당신은 객관적 AI입니다. {topic}에 대한 두 가지 의견을 종합하세요.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"낙관적 의견: {optimistic_opinion}\\n비관적 의견: {pessimistic_opinion}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "synthesize_chain = (\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "        \"topic\": itemgetter(\"topic\"),\n",
        "    }\n",
        "    | synthesize_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIWPMfaHgpk7"
      },
      "source": [
        "## 5.4. RunnablePassthrough―입력을 그대로 출력하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZEBEbd9gpk7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mFb4dTHgpk7"
      },
      "outputs": [],
      "source": [
        "!pip install tavily-python==0.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkeaFNJtgpk7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\\\n",
        "다음 문맥만을 고려해 질문에 답하세요.\n",
        "\n",
        "문맥: \"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "질문: {question}\n",
        "''')\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGANXxDTgpk7"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
        "\n",
        "retriever = TavilySearchAPIRetriever(k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRwWTmr4gpk8"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWMIB2fegpk8"
      },
      "source": [
        "### assign―RunnableParallel의 출력에 값 추가하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:14.820735Z",
          "iopub.status.busy": "2024-06-28T02:34:14.820536Z",
          "iopub.status.idle": "2024-06-28T02:34:20.477887Z",
          "shell.execute_reply": "2024-06-28T02:34:20.477378Z"
        },
        "id": "kuEaBuX1gpk8"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": retriever,\n",
        "} | RunnablePassthrough.assign(answer=prompt | model | StrOutputParser())\n",
        "\n",
        "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
        "pprint.pprint(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:20.479839Z",
          "iopub.status.busy": "2024-06-28T02:34:20.479664Z",
          "iopub.status.idle": "2024-06-28T02:34:27.723756Z",
          "shell.execute_reply": "2024-06-28T02:34:27.723236Z"
        },
        "id": "M2TujE69gpk8"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain = RunnableParallel(\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"context\": retriever,\n",
        "    }\n",
        ").assign(answer=prompt | model | StrOutputParser())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWCCNWebgpk8"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFWT_Wq6gpk8"
      },
      "source": [
        "#### <보충:pick>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:27.725885Z",
          "iopub.status.busy": "2024-06-28T02:34:27.725708Z",
          "iopub.status.idle": "2024-06-28T02:34:34.101909Z",
          "shell.execute_reply": "2024-06-28T02:34:34.101439Z"
        },
        "id": "JKm5b4NRgpk8"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"question\": RunnablePassthrough(),\n",
        "            \"context\": retriever,\n",
        "        }\n",
        "    )\n",
        "    .assign(answer=prompt | model | StrOutputParser())\n",
        "    .pick([\"context\", \"answer\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk5Xznb9gpk8"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzHTA4a6gpk8"
      },
      "source": [
        "### (칼럼) astream_events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdHIHS7mgpk9"
      },
      "outputs": [],
      "source": [
        "# Google Colab에서는 다음 코드의 \"async\" 부분에 \"Use of \"async\" not allowed outside of async function\"이라고 표시되지만, 오류 없이 실행할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:34.103973Z",
          "iopub.status.busy": "2024-06-28T02:34:34.103802Z",
          "iopub.status.idle": "2024-06-28T02:34:34.107769Z",
          "shell.execute_reply": "2024-06-28T02:34:34.107274Z"
        },
        "id": "c43P3RnWgpk9"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "async for event in chain.astream_events(\"서울의 현재 날씨는?\", version=\"v2\"):\n",
        "    print(event, flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xELg9Dj3gpk9"
      },
      "outputs": [],
      "source": [
        "async for event in chain.astream_events(\"서울의 현재 날씨는?\", version=\"v2\"):\n",
        "    event_kind = event[\"event\"]\n",
        "\n",
        "    if event_kind == \"on_retriever_end\":\n",
        "        print(\"=== 검색 결과 ===\")\n",
        "        documents = event[\"data\"][\"output\"]\n",
        "        for document in documents:\n",
        "            print(document)\n",
        "\n",
        "    elif event_kind == \"on_parser_start\":\n",
        "        print(\"=== 최종 출력 ===\")\n",
        "\n",
        "    elif event_kind == \"on_parser_stream\":\n",
        "        chunk = event[\"data\"][\"chunk\"]\n",
        "        print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J1ZES31gpk9"
      },
      "source": [
        "### (칼럼) Chat history와 Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPqsChLigpk9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATzS7K7Sgpk9"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "\n",
        "\n",
        "def respond(session_id: str, human_message: str) -> str:\n",
        "    chat_message_history = SQLChatMessageHistory(\n",
        "        session_id=session_id, connection=\"sqlite:///sqlite.db\"\n",
        "    )\n",
        "\n",
        "    ai_message = chain.invoke(\n",
        "        {\n",
        "            \"chat_history\": chat_message_history.get_messages(),\n",
        "            \"input\": human_message,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    chat_message_history.add_user_message(human_message)\n",
        "    chat_message_history.add_ai_message(ai_message)\n",
        "\n",
        "    return ai_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMFL6ZHagpk9"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "session_id = uuid4().hex\n",
        "\n",
        "output1 = respond(\n",
        "    session_id=session_id,\n",
        "    human_message=\"안녕하세요! 저는 존이라고 합니다!\",\n",
        ")\n",
        "print(output1)\n",
        "\n",
        "output2 = respond(\n",
        "    session_id=session_id,\n",
        "    human_message=\"제 이름을 알고 계신가요?\",\n",
        ")\n",
        "print(output2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}